{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영풍문고에서 베스트 셀러 엑셀 다운로드 버튼 클릭하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "url=''\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(url)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"rightArea01\"]/div[1]/div/dl/dt[5]/span/a').click()\n",
    "best=pd.read_excel('/BestSeller.xls')\n",
    "driver.close()  # 셀레니움 세션 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### headless 모드로 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option=webdriver.ChromeOptions()\n",
    "option.add_argument('headless') # 창을 드러내지 않고 크롤링 작업하기. -> 속도 빨라짐!\n",
    "# 크롬 창 자체에서 클릭 등 상호작용이 일어나는 크롤링의 경우 사용하면 안됨!\n",
    "url = ''\n",
    "driver = webdriver.Chrome(options=option)\n",
    "driver.get(url)\n",
    "title = driver.find_element(By.XPATH, '//*[@id=\"rightArea01\"]/div[2]/div[2]/ul/li/dl/b/dd[1]/strong[2]').text\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인크루트 데이터 직무 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job=[]\n",
    "com=[]\n",
    "category=[]\n",
    "info=[]\n",
    "\n",
    "for p in range(0, 1261,30):\n",
    "    \n",
    "    url=f'={p}'\n",
    "    req=requests.get(url)\n",
    "    content=req.content\n",
    "    soup= BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    if not soup.select('div.cl_top'):\n",
    "        break\n",
    "        # (len(soup.select('div.cl_md'))):\n",
    "    for i in range (len(soup.select('div.cl_md'))):\n",
    "        time.sleep(0.1)\n",
    "        job.append(soup.select('div.cl_top')[i*3+1].select('a')[0].text)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        com.append(soup.select('div.cl_top')[i*3].select('a')[0].text)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        category.append(re.sub('\\s+',' ',soup.select('div.cl_btm')[i*3+1].text))\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        info.append(re.sub('\\s+',' | ',soup.select('div.cl_md')[i].text))\n",
    "        \n",
    "res=pd.DataFrame([job, com, category, info], index=['공고명', '회사명', '직무', '채용정보'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
