{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# '사람인' 기업 정보 크롤링 - 헤드헌팅 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={'User-Agent' : ''}\n",
    "url=''\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(url)\n",
    "req=requests.get(url, headers = headers)\n",
    "content=req.content\n",
    "\n",
    "time.sleep(0.5)\n",
    "soup= BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame(columns=['공고명', '회사명', '채용정보-근무지', '채용정보-경력', '채용정보-학력', '채용정보-고용형태', '직무', '일자', '회사명2', '공고명2', '기업형태', '업종', '설립일', '매출액'])\n",
    "\n",
    "title=[]\n",
    "company=[]\n",
    "place = []\n",
    "lv= []\n",
    "edu= []\n",
    "category= []\n",
    "job= []\n",
    "day= []\n",
    "\n",
    "name=[]\n",
    "title2=[]\n",
    "company2=[]\n",
    "category=[]\n",
    "day=[]\n",
    "money=[]\n",
    "\n",
    "# 19페이지까지 크롤링\n",
    "for a in range(1,3):\n",
    "    \n",
    "    # 100개씩 크롤링\n",
    "    for p in range (1, 101):\n",
    "        print(f\"Processing page {p}\")  # 현재 페이지 번호 출력\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # (len(soup.select('div.cl_md'))) 길이만큼 반복\n",
    "        for i in range (len(soup.select('div.job_sector'))):\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # 공고명\n",
    "            title.append(soup.select('div.area_job')[i].select('a')[0].text)\n",
    "        # 회사명\n",
    "            company.append(re.sub('\\s+', '', soup.select('div.area_corp')[i].select('strong')[0].text))\n",
    "        # 채용정보- 근무지, 경력, 학력, 고용형태\n",
    "        # 정보가 없는 경우(결측치) 빈 셀로 처리\n",
    "            try:\n",
    "                place.append(soup.select('div.job_condition')[i].select('span')[0].text)\n",
    "            except IndexError:\n",
    "                place.append(' ')\n",
    "\n",
    "            lv.append(soup.select('div.job_condition')[i].select('span')[1].text)\n",
    "            edu.append(soup.select('div.job_condition')[i].select('span')[2].text)\n",
    "\n",
    "            try:\n",
    "                category.append(soup.select('div.job_condition')[i].select('span')[3].text)\n",
    "            except IndexError:\n",
    "                category.append(' ')\n",
    "                # 에러 확인 위해 print\n",
    "                print(f\"IndexError occurred at page {p}, index {i}\")\n",
    "        # 직무\n",
    "            job.append(re.sub('\\s+', ' ', soup.select('div.job_sector')[i].text).split(' 외 ')[0])\n",
    "        # 일자\n",
    "            day.append(soup.select('div.job_sector')[i].select('span.job_day')[0].text)\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # 개별 공고 클릭\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, f'//*[@id=\"recruit_info_list\"]/div[1]/div[{p}]/div[2]/h2/a').click() \n",
    "            except:\n",
    "                driver.find_element(By.XPATH, f'//*[@id=\"recruit_info_list\"]/div[1]/div[{p}]/div[1]/h2/a').click()\n",
    "            time.sleep(2)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            soup= BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            time.sleep(0.5)    \n",
    "\n",
    "            # 기업명\n",
    "            name.append(re.sub('\\s+', '', soup.select('div.title_inner')[0].select('a')[0].text))\n",
    "            # 구인글 제목\n",
    "            title.append(re.sub('\\s+', ' ', soup.select('h1.tit_job')[0].text))\n",
    "\n",
    "            # 정보가 없는 경우 빈 셀로 처리   \n",
    "\n",
    "            try:\n",
    "                # 'div.info_area' 클래스 내부의 모든 <dl> 요소를 선택합니다.\n",
    "                dl_elements = soup.select('div.info_area')[0].select('dl')\n",
    "\n",
    "                # 추출된 정보를 저장할 변수를 초기화합니다.\n",
    "                company_text, category_text, day_text, money_text = \"\", \"\", \"\", \"\"\n",
    "\n",
    "                # 각 <dl> 요소에 대해 반복합니다.\n",
    "                for dl_element in dl_elements:\n",
    "                    # <dl> 요소의 텍스트 내용을 추출합니다.\n",
    "                    dl_text = dl_element.text\n",
    "\n",
    "                    # 회사 형태 정보가 있는지 확인합니다.\n",
    "                    if '기업형태' in dl_text:\n",
    "                        # 회사 형태를 추출하고 선행/후행 공백을 제거합니다.\n",
    "                        company_text = dl_text.split('기업형태')[1].strip()\n",
    "\n",
    "                    # 업종 정보가 있는지 확인합니다.\n",
    "                    elif '업종' in dl_text:\n",
    "                        # 업종을 추출하고 선행/후행 공백을 제거합니다.\n",
    "                        category_text = dl_text.split('업종')[1].strip()\n",
    "\n",
    "                    # 설립일 정보가 있는지 확인합니다.\n",
    "                    elif '업력' in dl_text:\n",
    "                        # 설립일을 추출하고 선행/후행 공백을 제거합니다.\n",
    "                        day_text = re.sub('\\s+', ' ', dl_text).split('설립일')[1].strip()\n",
    "\n",
    "                    # 매출액 정보가 있는지 확인합니다.\n",
    "                    elif '매출액' in dl_text:\n",
    "                        # 매출액을 추출하고 선행/후행 공백을 제거합니다.\n",
    "                        money_text = re.sub('\\s+', ' ', dl_text).split('매출액')[1].strip()\n",
    "\n",
    "                # 추출된 정보를 각각의 리스트에 추가합니다.\n",
    "                # 만약 추출된 정보가 비어 있다면, 정렬을 유지하기 위해 빈 문자열을 추가합니다.\n",
    "                company.append(company_text if company_text else '')\n",
    "                category.append(category_text if category_text else '')\n",
    "                day.append(day_text if day_text else '')\n",
    "                money.append(money_text if money_text else '')\n",
    "\n",
    "            except IndexError:\n",
    "                # IndexError가 발생하는 경우 (예: 인덱스가 범위를 벗어난 경우), 정렬을 유지하기 위해 빈 문자열을 추가합니다.\n",
    "                company.append('')\n",
    "                category.append('')\n",
    "                day.append('')\n",
    "                money.append('')\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "            time.sleep(0.5)\n",
    "            driver.switch_to.window(driver.window_handles[-1]) \n",
    "\n",
    "            # 만약 행을 찾을 수 없다면, 더 이상 행이 없으므로 루프를 중지합니다.\n",
    "            if not soup.select('h1.tit_job'):\n",
    "                break\n",
    "        # 에러 발생 시 중지.\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    \n",
    "in_temp=pd.DataFrame([title, company, place, lv, edu, category, job, day, name, title2, company2, category, day, money], index=['공고명', '회사명', '채용정보-근무지', '채용정보-경력', '채용정보-학력', '채용정보-고용형태', '직무', '일자', '회사명2', '공고명2', '기업형태', '업종', '설립일', '매출액']).T\n",
    "in_result2=pd.concat([res, in_temp], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
